{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "# Our helper, with the functions: \n",
    "# plot_vector, plot_linear_transformation, plot_linear_transformations\n",
    "from plot_helper import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eckart–Young Theorem & Low Rank Approximation\n",
    "\n",
    "Consider the $4 \\times 4$ matrix\n",
    "\n",
    "$ A = \\begin{bmatrix} 1 & 2 & 3 & 6 \\\\\n",
    "                      2 & 5 & 7 & 10 \\\\\n",
    "                      3 & 9 & 12 & 14 \\\\\n",
    "                      4 & 7 & 9 & 15 \\\\\n",
    "                      \\end{bmatrix} $\n",
    "                      \n",
    "The rank of $A$ is $4$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3, 6],\n",
    "              [2, 5, 7, 10],\n",
    "              [3, 9, 12, 14],\n",
    "              [4, 7, 9, 15]])\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ has exactly four non-zero singular values, another justification for the fact that rank of $A$ is four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.94504943  2.81273628  0.77396598  0.05751823]\n"
     ]
    }
   ],
   "source": [
    "U, S, VT = numpy.linalg.svd(A)\n",
    "print(S)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last singular value $\\sigma_4 = 0.05751823$ is significantly smaller than the others and is close to zero. $A$ has full rank but is very close to a rank-$3$ matrix. By dropping the smallest singular value and its singular vectors in the decomposition, we arrive at a rank-$3$ truncation $\\tilde{A}_3$ of matrix $A$. \n",
    "\n",
    "\n",
    "$$ \\tilde{A_3} = \\sigma_1 u_1 v_1^T +  \\sigma_2 u_2 v_2^T +  \\sigma_3 u_3 v_3^T $$\n",
    "\n",
    "As we shall learn in the following section, the truncated matrix $\\tilde{A_3}$ is the best approximation to $A$ among all rank $3$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_trunc (matrix, level) : \n",
    "    U, S, VT = numpy.linalg.svd(matrix)\n",
    "    matrix = U[:,:level] @ numpy.diag(S[:level]) @ VT[:level,:]\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.01333778  1.9779498   3.0176867   5.99611842]\n",
      " [ 1.98096577  5.03146764  6.97475947 10.00553936]\n",
      " [ 3.00726293  8.98799282 12.00963108 13.99788633]\n",
      " [ 4.00058714  6.99902932  9.00077859 14.99982913]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "A_3 = svd_trunc(A, 3) \n",
    "\n",
    "print (A_3)\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4666304   2.18725118  2.90681548  5.85953327]\n",
      " [ 2.28645823  5.17252441  6.90003882  9.91348904]\n",
      " [ 3.00573676  8.98728813 12.01000437 13.9983462 ]\n",
      " [ 3.62585599  6.82600258  9.09243439 15.11274231]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "A_2 = svd_trunc(A, 2)\n",
    "\n",
    "print (A_2)\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.17291925  2.72752312  3.63917489  5.11820073]\n",
      " [ 2.25177822  5.23631714  6.98651232  9.82595603]\n",
      " [ 3.48539152  8.10497908 10.8140005  15.20900393]\n",
      " [ 3.24034329  7.53514042 10.05369803 14.13970093]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "A_1 = svd_trunc(A, 1)\n",
    "\n",
    "print(A_1)\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though $\\tilde{A}_3$ is rank-deficient, each entry is very close to the full-rank matrix $A$. The example might look trivial, but it demonstrates the important idea that we can sometimes approximate a higher-rank matrix with a reduced-rank matrix to save storage and transmission costs. This strategy is called *low-rank approximation*, and you will find that it's ubiquitous in scientific computing.\n",
    "\n",
    "**Warning**: Rounding errors may lead to small but non-zero singular values in a rank deficient matrix. Singular values that are smaller than a given tolerance are assumed to be numerically equivalent to zero, defining what is sometimes called the **effective rank**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Reminder  \n",
    "\n",
    "Suppose $A$ is an $m \\times n$ matrix with real entries. Perform the the singular value decomposition to obtain $ A = U\\Sigma V^T$. Recall that $U$ and $V$ are orthogonal matrices, and $\\Sigma$ is an $m\\times n$ diagonal matrix with entries $(\\sigma_{1}, \\sigma_{2}, \\cdots ,\\sigma_{n})$ such that $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_n\\geq 0$.\n",
    "\n",
    "\n",
    "\n",
    "Recall that the singular values of $A$ (i.e. the non-zero diagonal entries of $\\Sigma$) are the square-root of eigenvalues of $A^T \\, A$. The matrix $A^T \\, A$ is an $n \\times n$ matrix and, therefore, it has $n$ eigenvalues. We have as many singular values and singular vectors as the number of columns of $A$. This is because, we have $n$ = \"the number of columns of $A$\" is the dimension of the the domain of the linear transformation $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ (which is $ \\mathbb{R}^n$)  \n",
    "\n",
    "$V$ is an $n \\times n$ orthogonal matrix whose columns are eigenvectors of $A^T \\, A$. The columns of $V$ are called the *right singular vectors* of $A$.\n",
    "\n",
    "$U$ is an $m \\times m$ orthogonal matrix whose columns are eigenvectors of $A \\, A^T$. The columns of $U$ are called the *left singular vectors* of $A$.\n",
    "\n",
    "\n",
    "Suppose $r = \\mathrm{rank}(A)$. We have that $r \\le \\mathrm{min}(m,n)$. We analyze the three cases of the shape of SVDs based on the comparison of $m$ and $n$. In both cases, \n",
    "\n",
    "$$ A v_i = \\sigma_i u_i \\, \\text{for all} \\,  i \\le r \\, \\, \\text{and} \\, \\, A v_i =  0 \\, \\text{forall} \\, i > r $$ \n",
    "\n",
    "In all cases, we will have $r$ positive singular values in descending order: \n",
    "\n",
    "$$\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge \\sigma_r > 0  $$\n",
    "\n",
    "\n",
    "And in all cases, \n",
    "\n",
    "$$ A = \\sigma_1 u_1 v_1^T + \\ldots + \\sigma_r u_r v_r^T $$ \n",
    "\n",
    "The remaining singular values are zero. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Cases of SVD Shapes\n",
    "\n",
    "\n",
    "1. **More columns than rows**: If $m < n$ then the SVD of $A$ looks like \n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} & \\mathbf{u_2} & \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & & 0 & \\ldots & 0  \\\\\n",
    "    & \\sigma_2 & & & 0 & \\ldots & 0  \\\\\n",
    "    & & \\ddots  & & 0 & \\ldots & 0 \\\\\n",
    "    & & & \\sigma_m & 0 & \\ldots & 0\\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\mathbf{v_2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$m -r$ singular values are zero. In the diagonal matrix $\\Sigma$ we usually order the singular values in terms of their magnitude from the top to the bottom. Therefore, all the zero $\\sigma$'s come after all positive $\\sigma$'s. The above decomposition can be rewritten as\n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} & \\mathbf{u_2} & \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & & & 0 & \\ldots & 0  \\\\\n",
    "      & \\ddots  & &  & & 0 & \\ldots & 0 \\\\\n",
    "    & & \\sigma_r  & & &  \\vdots & \\ldots & \\vdots \\\\\n",
    "    & &  & \\ddots  & &  0 & \\ldots & 0 \\\\\n",
    "    & & & & 0 & 0 & \\ldots & 0\\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\mathbf{v_2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "On the diagonal entries whatever comes after $\\sigma_r$ is going to be zero, i.e. $\\sigma_{r+1} = \\ldots \\sigma_m = 0$.\n",
    "\n",
    "\n",
    "2. **More rows than columns**: If $m > n$ then the SVD of $A$ looks like \n",
    "\n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} & \\mathbf{u_2} & \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & &   \\\\\n",
    "      & \\ddots  & &  &  \\\\\n",
    "    & & \\sigma_r  & &  \\\\\n",
    "    & &  & \\ddots  &  \\\\\n",
    "    & & & & 0 \\\\\n",
    "    0 & & \\ldots & & 0 \\\\\n",
    "    \\vdots & & \\ddots & & \\vdots \\\\ \n",
    "    0 & & \\ldots & & 0 \\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Same number of rows as columns**: If $m = n$ then the SVD of $A$ looks like \n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} &  \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & &   \\\\\n",
    "      & \\ddots  & &  &  \\\\\n",
    "    & & \\sigma_r  & &  \\\\\n",
    "    & &  & \\ddots  &  \\\\\n",
    "    & & & & 0 \\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_m}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "where the diagonal matrix $\\Sigma$ is a square matrix.\n",
    "\n",
    "\n",
    "In all of the above cases: \n",
    "\n",
    "- $V$ is an  orthogonal $n \\times n$ matrix whose columns $v_i$ are perpendicular to each other. Note that $n - r$ of the vectors $v_i$ are in the null-space of $A$, indeed $A v_{r +1} = \\ldots = A v_n = 0$. \n",
    "\n",
    "- $m - r$ vectors $u_{r + 1}, \\ldots, u_m$ are in the null-space of $A^T$, since \n",
    "$$ A^T = V \\Sigma^T U^T$$ \n",
    "and therefore, for all $i > r$ \n",
    "$$A^T u_i = \\sigma_1 v_1 u_1^T u_i + \\ldots + \\sigma_r v_r u_r^T u_i  = 0$$ \n",
    "because all the inner products $\\langle u_1, u_i  \\rangle  = u_1^T u_i, \\ldots, \\langle u_r, u_i  \\rangle = u_r^T u_i$ are zero. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SVD Truncation of Matrices \n",
    "\n",
    "Suppose $k \\le r = \\mathrm{rank}(A)$. The SVD **$k$-truncation** of $A$ is given by the following $k$-truncated sum of the rank one matrices. \n",
    "\n",
    "$$\n",
    "\\tilde{A_k} := \\sum_{i=1}^k \\sigma_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "Equivalently, $\\tilde{A_k}$ is equal to the product $U_k \\Sigma_k V_k$ of shapes ${m \\times k}$, ${k \\times k}$ and ${k \\times m}$. \n",
    "\n",
    "$$\n",
    "  \\tilde{A_k} =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} &  \\dots & \\mathbf{u_k}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & &    \\\\\n",
    "      & \\ddots  &     \\\\\n",
    "    & & \\sigma_k     \\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_k}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that this is in the reduced form and the full SVD of $\\tilde{A_k}$ should have square matrices for $U$ and $V$. The full SVD of $\\tilde{A_k} = \\tilde{U_k} \\tilde{\\Sigma_k} \\tilde{V_k}$ is given as \n",
    "\n",
    "$$\n",
    "  \\tilde{A_k} =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} &  \\dots & \\mathbf{u_k} & \\dots & \\mathbf{u_m} \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & &  0 & \\ldots & 0  \\\\\n",
    "      & \\ddots  & &   0 & \\ldots & 0 \\\\\n",
    "    & & \\sigma_k  &   \\vdots & \\ldots & \\vdots \\\\\n",
    "    0  & \\ldots & 0 &  0 & \\ldots & 0\\\\\n",
    "    \\vdots & \\ldots & 0 & 0  &  \\ldots & 0 \\\\\n",
    "    0 & \\ldots & 0 & 0 &  \\ldots & 0\\\\\n",
    "    \\end{bmatrix}    \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_k}^T \\\\ \n",
    "    \\vdots \\\\ \n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix} = \n",
    "    U \\tilde{\\Sigma_k} V^T\n",
    "$$\n",
    "\n",
    "In practice though, we would like to not have to store and transmit all the useless $0$ in the matrix $\\Sigma$ above. That is what the reduced SVD as opposed to full SVDs are used in practice.  \n",
    "\n",
    "\n",
    "The $k$-truncated matrix $\\tilde{A_k}$ contains the $k$ most important pieces of $A$, namely $\\sigma_1 u_1 v_1^T, \\ldots, \\sigma_k u_k v_k^T$. The Eckart–Young theorem tells us that the $k$-truncated matrix $\\tilde{A_k}$ is the best rank-$k$ approximation to $A$ in the sense that $ \\| A - \\tilde{A_k} \\|$ is minimized among all rank $\\le k$ matrix. In other words, for all matrices $B$ of rank $k$ or less,  \n",
    "\n",
    "$$  \\| A - \\tilde{A_k} \\| \\le \\| A - B \\| $$ \n",
    "\n",
    "The norm for which this equality works is one of the followings: \n",
    "- $L^2$ norm of a matrix $M =$  $\\| M \\|_2 = \\sigma_1$ \n",
    "- The Frobenius norm of a matrix $\\| M \\|_\\text{F} = \\big(\\sum_{i=1}^{r} \\sigma_i^2\\big)^{1/2} $  \n",
    "- The nuclear norm of a matrix $\\| M \\|_\\text{F} = \\sum_{i=1}^{r} \\sigma_i $  (check that this is indeed a norm. Is this norm induced by an inner product? What is this inner product?)\n",
    "\n",
    "Note that the matrix $\\tilde{A_k}$ has rank $k$ since its SVD returns exactly non-zero singular values. Therefore, \n",
    "\n",
    "\n",
    "$$ \\|A - \\tilde{A_k} \\| = \\min_{\\operatorname{rank}(B) \\leq k} \\| A - B \\|  $$\n",
    "\n",
    "We can actually calculate this minimum: \n",
    "\n",
    "- For the $L^2$-norm we have: \n",
    "$ \n",
    "\\|A - \\tilde{A_k} \\|_2  =  \\| U \\Sigma V^T - U \\tilde{\\Sigma_k} V^T \\|_2 = \\|  U (\\Sigma - \\tilde{\\Sigma_k} ) V^T \\| = \\|_2 = \\| \\Sigma - \\tilde{\\Sigma_k} \\|_2 = \\sigma_{k + 1}  \n",
    "$\n",
    "\n",
    "- For the Frobenius norm we have \n",
    "$\n",
    "\\|A - \\tilde{A_k} \\|_\\text{F}  =  \\big(\\sigma_{k+1}^2 + \\ldots + \\sigma_r^2 \\big)^{1/2} \n",
    "$\n",
    "\n",
    "- For the nuclear norm we have \n",
    "$\n",
    "\\|A - \\tilde{A_k} \\|_\\text{nuclear}  =  \\sigma_{k+1} + \\ldots + \\sigma_r\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Eckart-Young for the $L^2$ norm\n",
    "\n",
    "\n",
    "We want to show that $\\|A - \\tilde{A_k} \\| = \\min_{\\operatorname{rank}(B) \\leq k} \\| A - B \\|$. Take a matrix $B$ of rank at most $k$. If $k = r = \\mathrm{rank}(A)$ then \n",
    "\n",
    "$$ \\min_{\\operatorname{rank}(B) \\leq k} \\| A - B \\| =  \\min_{\\operatorname{rank}(B) \\leq \\operatorname{rank}(A)} \\| A - B \\| = 0 = \\|A - \\tilde{A_r} \\|$$\n",
    "since we can $B$ to be $A$ itself. Therefore, we only need to prove the theorem in case $k < r \\le n$. Therefore $n - k \\ge 1$. By the rank-nullity theorem, the null-space of $B$ has at least $n-k \\ge 1$ dimensions. Consider the first $k + 1$ right singular vectors $v_1, \\ldots , v_{k+1}$ of $A$ and let $W_{k+1}$ be the space spanned by these vectors. $\\mathrm{dim}(W) = k+1$. By an easy dimension analysis, there must be a non-zero vector $w$ in the intersection $W \\cap \\mathrm{null}(B)$. Therefore, there must be a nontrivial linear combination of the first $k+1$ columns of $V$, i.e.,\n",
    "\n",
    "$$\n",
    "w = c_1 v_1 + \\cdots+ c_{k+1} v_{k+1},\n",
    "$$\n",
    "\n",
    "such that $B w = 0$. Without loss of generality, we can scale $w$ so that $\\|w\\|_2 = 1$ or (equivalently) $c_1^2+\\cdots +c_{k+1}^2 = 1$. Therefore,\n",
    "\n",
    "$$\n",
    "\\|A-B\\|_2^2 = \\max_{w, \\, \\| w \\| = 1} \\|(A-B)w\\|_2^2  \\geq \\|(A-B)w\\|_2^2 = \\|Aw\\|_2^2 = c_1^2\\sigma_1^2+\\cdots+c_{k+1}^2\\sigma_{k+1}^2\\geq \\sigma_{k+1}^2 = \\| A - \\tilde{A_k} \\|_2^2.\n",
    "$$\n",
    "\n",
    "The result follows by taking the square root of both sides of the above inequality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the example at the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle 0.05751823344612$"
      ],
      "text/plain": [
       "0.05751823344612"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A - A_3, ord=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which verifies Eckart-Young theorem, since `0.05751823344612` is exactly the fourth (and the last) singular value of $A$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle 0.773965982529115$"
      ],
      "text/plain": [
       "0.7739659825291152"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A - A_2, ord=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is the third largest singular value of $A$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frobenius norm $\\|A - \\tilde{A_k} \\|_\\text{F}$  can also be minimized too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle 0.776100308781681$"
      ],
      "text/plain": [
       "0.7761003087816812"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A - A_2, ord='fro')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Eckart-Young for the Frobenius Norm using Calculus\n",
    "\n",
    "\n",
    "From HW2 we know that the Frobenius norm is induced by the following inner product on the linear space of matrices of a given size: \n",
    "\n",
    "$$ \\| A \\|^2_{\\text{F}} = \\langle A , A \\rangle  = \\mathrm{tr}(A^T A) $$ \n",
    "\n",
    "The inner-product function $\\langle \\, , \\, \\rangle : \\mathbb{R}^{m \\times n} \\times \\mathbb{R}^{m \\times n} \\to \\mathbb{R^{\\ge 0}}$ is continuous and differentiable in both variables. So max/min can be obtained by setting the partial derivatives to zero. \n",
    "\n",
    "To minimize $\\|A - B \\|_\\text{F}$ is the same to minimize \n",
    "$$\\|A - B \\|^2_\\text{F} = \\langle A - B , A  - B \\rangle = \\mathrm{tr}((A^T-B^T) (A-B)) = \\mathrm{tr}(A^T A) - 2 \\mathrm{tr}(A^T B) + \\mathrm{tr}(B^T B).$$ \n",
    "\n",
    "\n",
    "To use this we need to decompose $B$ into a matrix $C$ \n",
    "\n",
    "\n",
    "####  The Column-Row Decomposition (CR Decomposition)\n",
    "\n",
    "Given an $m \\times n$ matrix $A$ list the independent columns of $A$ from the left to right and put them as the columns of a new matrix $C$. Then each column of $A$ is a linear combination of the columns of $C$, and therefore, \n",
    "\n",
    "$$ A = C \\, R$$ \n",
    "\n",
    "for some matrix $R$. If $A$ has $r$ independent columns ($k \\le n$) then $C$ is an $m \\times r$ matrix and $R$ is an $r \\times n$ matrix. The columns of $C$ are the first $r$ linearly independent columns of $A$. All the linear combination action comes from $R$. Note that $\\mathrm{rank}(A) = r$ is the number of columns of $C$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "1 & 3 & 8 \\\\ \n",
    "1 & 2 & 6 \\\\ \n",
    "0 & 1 & 2 \\\\\n",
    "\\end{bmatrix} = \n",
    "\n",
    "\\begin{bmatrix} \n",
    "1 & 3 \\\\ \n",
    "1 & 2 \\\\ \n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 2 \\\\ \n",
    "0 & 1 & 2 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompose $C$ further using SVD as $U \\Sigma V^T $. Therefore, $A = C R =  (U \\Sigma) (V^T R)$.  Let's write $C' = U \\Sigma$ and $R' = V^T R$. Now, the columns of $C'$ are orthogonal and therefore, $C'^T C'$ is diagonal.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "\n",
    "To implement CR decomposition in Python we use the reduced row echelon form (`rref`). Recall the requirements for a matrix to be in reduced row echelon form:\n",
    "\n",
    "- The first nonzero entry in any nonzero row is a 1, called the leading 1.\n",
    "- All of the other elements in a column containing a leading 1 are zero.\n",
    "- Each leading 1 occurs later in its row than the leading 1's in earlier rows.\n",
    "- All zero rows are below the nonzero ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 1 & 3\\\\2 & 3 & 4 & 7\\\\-1 & -3 & -3 & -4\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡1   0   1   3 ⎤\n",
       "⎢              ⎥\n",
       "⎢2   3   4   7 ⎥\n",
       "⎢              ⎥\n",
       "⎣-1  -3  -3  -4⎦"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "sympy.init_printing(use_latex = 'mathjax') #configures the display of mathematics\n",
    "\n",
    "\n",
    "from sympy import init_printing\n",
    "init_printing() \n",
    "\n",
    "from sympy import * \n",
    "  \n",
    "M = Matrix([[1, 0, 1, 3], [2, 3, 4, 7], [-1, -3, -3, -4]])\n",
    "# print(\"Matrix : {} \".format(M))\n",
    "   \n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 1 & 3\\\\0 & 1 & \\frac{2}{3} & \\frac{1}{3}\\\\0 & 0 & 0 & 0\\end{matrix}\\right], \\  \\left( 0, \\  1\\right)\\right)$"
      ],
      "text/plain": [
       "⎛⎡1  0   1    3 ⎤        ⎞\n",
       "⎜⎢              ⎥        ⎟\n",
       "⎜⎢0  1  2/3  1/3⎥, (0, 1)⎟\n",
       "⎜⎢              ⎥        ⎟\n",
       "⎝⎣0  0   0    0 ⎦        ⎠"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sympy.rref() method \n",
    "M_rref = M.rref()  \n",
    "M_rref    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matrix().rref()` returns a tuple of two elements. The first is the reduced row echelon form, and the second is a tuple of indices of the pivot columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcfds_LinAlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
